# -*- coding: utf-8 -*-
"""single_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nY53wNQnvtveUh-RUuDDHxI_yDbg5oY7
"""

!pip install transformers datasets accelerate bitsandbytes --quiet

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from datasets import load_dataset, Dataset
import time
import torch
import pandas as pd
import re

MODELS = {
    "llama3": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "mistral": "mistralai/Mistral-7B-Instruct-v0.3",
    "gemma": "google/gemma-2-9b-it",
    "phi3": "microsoft/phi-3-medium-4k-instruct",
    "qwen2.5": "Qwen/Qwen2.5-14B-Instruct",
    "hermes": "NousResearch/Hermes-3-Llama-3.1-8B"
}
OUTPUT_DIR = "/mnt/new_home/cbd/semantica"
PROMPT_STYLES = ["simple", "cot"]

dataset = load_dataset("tau/commonsense_qa", split="validation")

def format_prompt(example, style="simple"):
    question = example["question"]
    choices = example["choices"]
    formatted_choices = "\n".join([f"{label}. {text}" for label, text in zip(choices["label"], choices["text"])])

    if style == "simple":
        return (
            f"Answer the following multiple-choice question with the correct letter (A–E), there is only one correct answer:\n"
            f"{question}\n\n{formatted_choices}\n\nThe correct answer is:"
        )
    elif style == "cot":
        return (
            f"Answer the following multiple-choice question with the correct letter (A–E), there is only one correct answer.\n"
            f"{question}\n\n{formatted_choices}\n\n"
            f"Think step by step.\n"
            f"The correct answer is:"
        )
    else:
        raise ValueError(f"Unknown prompt style: {style}")

from huggingface_hub import login
login(token="***********")

MODEL_CACHE = {}

def load_model(model_id):
    if model_id in MODEL_CACHE:
        return MODEL_CACHE[model_id]

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    MODEL_CACHE[model_id] = (tokenizer, model)
    return tokenizer, model

def extract_letter(response):
    """
    Extracts the answer letter (A–E) from a model's response.
    Handles formats like:
    - The correct answer is: B
    - Correct answer is: (B)
    - Final Answer: B
    - Answer: **B. bookstore**
    """

    match = re.search(r"(?i)(the\s+)?correct answer is\s*[:\-]?\s*\**\(?([A-E])\)?\**", response)
    if match:
        return match.group(2).upper()

    match = re.search(r"(?i)final answer\s*[:\-]?\s*\**\(?([A-E])\)?\**", response)
    if match:
        return match.group(1).upper()

    match = re.search(r"(?i)answer\s*[:\-]?\s*\**\(?([A-E])\)?", response)
    if match:
        return match.group(1).upper()

    lines = response.strip().splitlines()
    last_lines = " ".join(lines[-3:])
    fallback = re.findall(r"\b([A-E])\b", last_lines)
    if fallback:
        return fallback[-1]

    return None

loaded_models = {
    name: load_model(model_id)
    for name, model_id in MODELS.items()}

tokenizer, model = load_model("NousResearch/Hermes-3-Llama-3.1-8B")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

batch_size = 64
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="simple") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)

df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

import gc
import torch
del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("microsoft/phi-3-medium-4k-instruct")
tokenizer.pad_token = tokenizer.eos_token

batch_size = 16
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="simple") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=150, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

import gc
import torch
del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("Qwen/Qwen2.5-14B-Instruct")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

batch_size = 16
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="simple") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

import gc
import torch
del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("mistralai/Mistral-7B-Instruct-v0.3")
tokenizer.pad_token = tokenizer.eos_token

batch_size = 64
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="simple") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

import gc
import torch
del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("meta-llama/Meta-Llama-3.1-8B-Instruct")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

batch_size = 64
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="simple") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("google/gemma-2-9b-it")
tokenizer.pad_token = tokenizer.eos_token

batch_size = 16
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="simple") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=64, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("mistralai/Mistral-7B-Instruct-v0.3")
tokenizer.pad_token = tokenizer.eos_token

batch_size = 16
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="cot") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=400, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("meta-llama/Meta-Llama-3.1-8B-Instruct")
tokenizer.pad_token = tokenizer.eos_token

batch_size = 8
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="cot") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=200, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("google/gemma-2-9b-it")
tokenizer.pad_token = tokenizer.eos_token

batch_size = 4
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="cot") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=400, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("NousResearch/Hermes-3-Llama-3.1-8B")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

batch_size = 64
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="cot") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=400, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("microsoft/phi-3-medium-4k-instruct")
tokenizer.pad_token = tokenizer.eos_token

batch_size = 64
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="cot") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=400, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")

del model
del tokenizer
gc.collect()
torch.cuda.empty_cache()

tokenizer, model = load_model("Qwen/Qwen2.5-14B-Instruct")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

batch_size = 64
results = []
start = time.time()

for i in range(0, len(dataset), batch_size):
    batch = dataset.select(range(i, min(i + batch_size, len(dataset))))
    prompts = [format_prompt(ex, style="cot") for ex in batch]

    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=400, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)

    for ex, output in zip(batch, outputs):
        response = tokenizer.decode(output, skip_special_tokens=True)
        pred = extract_letter(response)
        results.append({
            "question": ex["question"],
            "correct_answer": ex["answerKey"],
            "model_response": response,
            "predicted_letter": pred,
            "correct": pred == ex["answerKey"]
        })

end = time.time()
df = pd.DataFrame(results)
df.to_csv(OUTPUT_DIR, index=False)

print(f"\nAccuracy: {df['correct'].mean():.2%} on {len(df)} examples")
print(f"Time: {end - start:.2f} sec")
