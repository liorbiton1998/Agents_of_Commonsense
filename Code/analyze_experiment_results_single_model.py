# -*- coding: utf-8 -*-
"""analyze_experiment_results_single_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hyc10dUv8ilSU9U1AwnB6PXBhuOHybvZ
"""

import pandas as pd
from itertools import combinations, product

base_path = "/mnt/new_home/cbd/semantica"

all_files = {
    "phi_simple": base_path + "phi_simple.csv",
    "qwen_simple": base_path + "Qwen_simple.csv",
    "mistral_simple": base_path + "mistral_simple.csv",
    "llama_simple": base_path + "llama_simple.csv",
    "gemma_simple": base_path + "gemma_simple.csv",
    "hermes_simple": base_path + "Hermes_simple.csv",
    "mistral_cot": base_path + "mistral_cot.csv",
    "llama_cot": base_path + "llama_cot.csv",
    "gemma_cot": base_path + "gemma_cot.csv"
}

dfs = {name: pd.read_csv(path) for name, path in all_files.items()}

accuracy = {
    name: (df["predicted_letter"] == df["correct_answer"]).mean()
    for name, df in dfs.items()
}

accuracy_df = pd.DataFrame.from_dict(accuracy, orient="index", columns=["Accuracy"])
accuracy_df.index.name = "Model"

print("Accuracy Table")
print(accuracy_df.sort_values("Accuracy", ascending=False).round(4))

simple_model_names = [name for name in dfs.keys() if name.endswith("_simple")]
comparison_simple = dfs[simple_model_names[0]][["question", "correct_answer"]].copy()
for name in simple_model_names:
    comparison_simple[name] = dfs[name]["predicted_letter"]

pairwise_agreements = []
for m1, m2 in combinations(simple_model_names, 2):
    agree = (comparison_simple[m1] == comparison_simple[m2]).mean()
    pairwise_agreements.append((f"{m1} ↔ {m2}", agree))

print("\nPairwise Agreement (Sorted)")
for pair, score in sorted(pairwise_agreements, key=lambda x: x[1], reverse=True):
    print(f"{pair}: {score:.2%}")

all_agree = comparison_simple[simple_model_names].nunique(axis=1) == 1
full_agreement_rate = all_agree.mean()
print(f"\nAll {len(simple_model_names)} models agree: {full_agreement_rate:.2%}")

def majority_or_qwen(row):
    # Get predictions from all _simple models in the row
    preds = [row[col] for col in simple_model_names]
    counts = pd.Series(preds).value_counts()

    if counts.iloc[0] >= 4:
        return counts.idxmax()
    else:
        return row["qwen_simple"]

comparison_simple["final_vote"] = comparison_simple.apply(majority_or_qwen, axis=1)
vote_accuracy = (comparison_simple["final_vote"] == comparison_simple["correct_answer"]).mean()
print(f"Majority Vote Accuracy (fallback to Qwen): {vote_accuracy:.2%}")

comparison_simple

def weighted_vote(row, model_weights):
    preds = []
    for model, weight in model_weights.items():
        preds.extend([row[model]] * weight)
    counts = pd.Series(preds).value_counts()
    return counts.idxmax()

model_weights_grid = {
    "qwen_simple": [1, 2, 3],
    "gemma_simple": [1, 2, 3],
    "phi_simple": [1, 2],
    "llama_simple": [1],
    "mistral_simple": [1],
    "hermes_simple": [1]
}

weight_combinations = list(product(*model_weights_grid.values()))
model_names = list(model_weights_grid.keys())

results = []

for weights in weight_combinations:
    def custom_vote(row):
        preds = []
        for model, w in zip(model_names, weights):
            preds.extend([row[model]] * w)
        counts = pd.Series(preds).value_counts()
        return counts.idxmax()

    col_name = f"vote_{'_'.join(str(w) for w in weights)}"
    comparison_simple[col_name] = comparison_simple.apply(custom_vote, axis=1)
    acc = (comparison_simple[col_name] == comparison_simple["correct_answer"]).mean()
    results.append((weights, acc))

sorted_results = sorted(results, key=lambda x: x[1], reverse=True)

print("Top Weighted Voting Strategies:")
for weights, acc in sorted_results:
    weight_str = ", ".join(f"{m}={w}" for m, w in zip(model_names, weights))
    print(f"{weight_str} → Accuracy: {acc:.4%}")

def any_model_correct(row):
    correct = row["correct_answer"]
    preds = [row[col] for col in simple_model_names]
    return correct in preds

comparison_simple["any_correct"] = comparison_simple.apply(any_model_correct, axis=1)

upper_bound_acc = comparison_simple["any_correct"].mean()
print(f"Upper Bound Accuracy (if any simple model is correct): {upper_bound_acc:.2%}")